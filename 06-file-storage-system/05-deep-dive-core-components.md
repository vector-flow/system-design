# Minutes 21-35: Deep Dive - Core Components (15 min)

## **Why This Matters**

This is where you **differentiate yourself** from other candidates. Anyone can draw boxes and arrowsâ€”senior engineers understand the nuances:

- Implementation details that matter
- Trade-offs at the component level
- Edge cases and failure scenarios
- Performance optimizations

**Golden Rule:** Go deep on 2-3 components. Show mastery, not breadth.

-----

## **Your Opening (15 seconds)**

**What to say:**

> â€œNow letâ€™s dive deep into the core components. Iâ€™ll focus on three critical areas: file chunking and storage, metadata management, and synchronization. These are the heart of the system and have the most interesting technical challenges.â€

-----

# **DEEP DIVE 1: File Chunking & Storage (5 minutes)**

## **Part A: Chunking Strategy (90 seconds)**

### **Why Chunk Files?**

**Start by establishing the reasoning:**

> â€œFirst, why do we chunk files at all? Four main reasons:â€

```
BENEFITS OF CHUNKING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
1. DEDUPLICATION
   - Same chunk across files stored once
   - User has 100 photos with same background
   - Background chunk stored once, saves 99% space
   
2. DELTA SYNC
   - Only upload modified chunks
   - Edit 1KB in 1GB file â†’ upload 4MB chunk
   - Saves bandwidth and time
   
3. PARALLEL UPLOADS/DOWNLOADS
   - 10 chunks Ã— 10 parallel connections
   - Saturate bandwidth better
   - Faster for large files
   
4. RESUME CAPABILITY
   - Upload fails at 80%
   - Resume from last successful chunk
   - No need to restart entire file
```

-----

### **Chunk Size Decision (60 seconds)**

**Present the trade-off analysis:**

```
CHUNK SIZE ANALYSIS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TOO SMALL (512 KB):
âœ— Too many chunks â†’ metadata explosion
âœ— More network overhead (headers per chunk)
âœ— More dedup lookups required
Example: 1GB file = 2,000 chunks

TOO LARGE (64 MB):
âœ— Less deduplication benefit
âœ— Poor delta sync (change 1 byte â†’ re-upload 64MB)
âœ— Harder to parallelize effectively
Example: 1GB file = 16 chunks

SWEET SPOT (4 MB):
âœ“ Good deduplication granularity
âœ“ Efficient delta detection
âœ“ ~250 chunks for 1GB file (manageable)
âœ“ Standard in industry (Dropbox uses 4MB)
âœ“ Good parallel upload performance

DECISION: 4 MB chunks
```

**Narrate:**

> â€œWeâ€™ll use 4MB chunks. This is battle-testedâ€”Dropbox, Google Drive, and OneDrive all use similar sizes. It balances deduplication benefits with metadata overhead.â€

-----

### **Chunking Algorithm (90 seconds)**

**Explain the implementation:**

> â€œNow, how do we actually chunk files? Two approaches:â€

**Approach 1: Fixed-Size Chunking (Simple)**

```python
# Pseudo-code
def fixed_chunk(file_path, chunk_size=4MB):
    chunks = []
    with open(file_path, 'rb') as f:
        chunk_id = 0
        while True:
            data = f.read(chunk_size)
            if not data:
                break
            
            # Calculate hash for deduplication
            chunk_hash = sha256(data)
            
            chunks.append({
                'id': chunk_id,
                'hash': chunk_hash,
                'size': len(data),
                'offset': chunk_id * chunk_size
            })
            chunk_id += 1
    
    return chunks
```

**Problem with fixed chunking:**

> â€œIf user inserts 1 byte at start of file, ALL chunks shift. Every chunk hash changes â†’ no deduplication benefit!â€

```
BEFORE INSERT:
File: [AAAA][BBBB][CCCC][DDDD]
       â†“     â†“     â†“     â†“
Hash: h1    h2    h3    h4

AFTER INSERT 'X' AT START:
File: [XAAA][ABBB][BCCC][CDDD][D...]
       â†“     â†“     â†“     â†“
Hash: h5    h6    h7    h8  â† All different!
```

-----

**Approach 2: Content-Defined Chunking (CDC) (Better)**

```python
# Pseudo-code using Rabin fingerprinting
def content_defined_chunk(file_path, avg_size=4MB):
    chunks = []
    with open(file_path, 'rb') as f:
        window = []
        current_chunk = []
        
        while True:
            byte = f.read(1)
            if not byte:
                break
            
            current_chunk.append(byte)
            window.append(byte)
            
            # Rolling hash (Rabin fingerprint)
            fingerprint = rolling_hash(window)
            
            # Check if this is a chunk boundary
            # Using low-order bits for boundary detection
            if (fingerprint & 0xFFF) == 0xFFF or len(current_chunk) >= 8MB:
                # Found boundary or max size reached
                chunk_data = bytes(current_chunk)
                chunk_hash = sha256(chunk_data)
                
                chunks.append({
                    'hash': chunk_hash,
                    'size': len(chunk_data),
                    'data': chunk_data
                })
                
                current_chunk = []
        
        # Handle last chunk
        if current_chunk:
            chunks.append(create_chunk(current_chunk))
    
    return chunks
```

**How CDC works:**

```
CONCEPT:
- Slide a window through file bytes
- Calculate rolling hash at each position
- When hash matches pattern (e.g., last 12 bits = all 1s)
  â†’ This is a chunk boundary
- Probability of boundary: 1/4096 â†’ avg 4KB chunks

ADVANTAGE:
File: [AAAA | BBBB | CCCC | DDDD]
              â†‘           â†‘
        Boundaries based on content

After insert:
File: [X | AAAA | BBBB | CCCC | DDDD]
       â†‘    â†‘           â†‘
       New  Same boundaries maintained!
       
Result: Only first chunk is new, rest are deduplicated!
```

**Decision:**

> â€œFor production, weâ€™d use **Content-Defined Chunking with Rabin fingerprinting**. Itâ€™s more complex but gives much better deduplication, especially for incremental file changes. This is what Dropbox uses.â€

-----

## **Part B: Deduplication Strategy (90 seconds)**

### **How Deduplication Works:**

```
DEDUPLICATION FLOW:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. CLIENT CHUNKS FILE
   vacation.jpg â†’ 13 chunks

2. CALCULATE CHUNK HASHES
   Chunk 0: hash â†’ a3f5c8d...
   Chunk 1: hash â†’ 7b2e9f1...
   ...

3. CHECK WHICH CHUNKS EXIST
   API call: POST /api/chunks/check
   Request: ["a3f5c8d...", "7b2e9f1...", ...]
   
4. SERVER QUERIES DEDUP TABLE
   SELECT chunk_hash FROM chunks 
   WHERE chunk_hash IN (...)
   
5. SERVER RESPONDS
   Response: {
     "existing": ["a3f5c8d...", "9c4d2a1..."],
     "needed": ["7b2e9f1...", "e8f3b7c..."]
   }

6. CLIENT UPLOADS ONLY NEEDED CHUNKS
   - Skip existing chunks (saves bandwidth!)
   - Upload only new chunks
   
7. SERVER CREATES FILE RECORD
   file_id â†’ [chunk_ref_1, chunk_ref_2, ...]
   Multiple files can reference same chunks!
```

-----

### **Deduplication Database Schema:**

```sql
-- Chunk storage table (deduplicated)
CREATE TABLE chunks (
    chunk_hash VARCHAR(64) PRIMARY KEY,  -- SHA-256 hash
    size_bytes INTEGER NOT NULL,
    storage_path VARCHAR(512) NOT NULL,  -- S3 key
    ref_count INTEGER DEFAULT 1,         -- Reference counting
    created_at TIMESTAMP DEFAULT NOW(),
    last_accessed TIMESTAMP,
    
    INDEX idx_last_accessed (last_accessed)  -- For cleanup
);

-- File to chunks mapping
CREATE TABLE file_chunks (
    file_id BIGINT NOT NULL,
    chunk_hash VARCHAR(64) NOT NULL,
    chunk_index INTEGER NOT NULL,       -- Order in file
    chunk_offset BIGINT NOT NULL,       -- Byte offset in file
    
    PRIMARY KEY (file_id, chunk_index),
    FOREIGN KEY (chunk_hash) REFERENCES chunks(chunk_hash),
    FOREIGN KEY (file_id) REFERENCES files(file_id) ON DELETE CASCADE
);
```

**Key design decisions:**

> â€œNotice a few things:
> 
> 1. **Content-addressable storage**: Chunk hash is the primary key
> 1. **Reference counting**: Track how many files use each chunk
> 1. **Garbage collection**: When ref_count hits 0, chunk can be deleted
> 1. **Separate mapping table**: Same chunk can be in multiple files/positionsâ€

-----

### **Deduplication Effectiveness (30 seconds)**

**Show the math:**

```
DEDUPLICATION SAVINGS EXAMPLE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Scenario: 100M users, each has:
- 5 copies of company logo (1MB)
- 3 versions of presentation (50MB each)
- Similar document templates

WITHOUT DEDUP:
100M users Ã— 5 logos Ã— 1MB = 500 TB
100M users Ã— 3 presentations Ã— 50MB = 15 PB
Total: ~15.5 PB

WITH DEDUP:
Company logo: Stored once Ã— 1MB = 1 MB
Presentation: ~70% same chunks
Deduplicated storage: ~5 PB

SAVINGS: 67% reduction â†’ Massive cost savings!
```

-----

## **Part C: Storage Architecture (90 seconds)**

### **Multi-Tier Storage Strategy:**

```
STORAGE TIERS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOT TIER (Recent/Frequently Accessed)      â”‚
â”‚  - SSD-backed storage                       â”‚
â”‚  - Low latency (~10ms)                      â”‚
â”‚  - Higher cost ($0.10/GB/month)             â”‚
â”‚  - TTL: 30 days since last access           â”‚
â”‚  - Example: S3 Standard                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ (Auto-transition after 30 days)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WARM TIER (Less Frequent Access)           â”‚
â”‚  - Standard HDD storage                     â”‚
â”‚  - Medium latency (~100ms)                  â”‚
â”‚  - Medium cost ($0.02/GB/month)             â”‚
â”‚  - TTL: 90 days since last access           â”‚
â”‚  - Example: S3 Infrequent Access            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â”‚ (Auto-transition after 90 days)
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COLD TIER (Archive)                        â”‚
â”‚  - Tape/slow HDD                            â”‚
â”‚  - High latency (~hours for retrieval)      â”‚
â”‚  - Low cost ($0.004/GB/month)               â”‚
â”‚  - Rarely accessed                          â”‚
â”‚  - Example: S3 Glacier                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Cost calculation:**

```
COST SAVINGS WITH TIERING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total storage: 15 PB

Distribution:
- Hot (recent):  20% â†’ 3 PB Ã— $0.10 = $300K/month
- Warm:          50% â†’ 7.5 PB Ã— $0.02 = $150K/month  
- Cold:          30% â†’ 4.5 PB Ã— $0.004 = $18K/month

Total: $468K/month

Without tiering (all hot):
15 PB Ã— $0.10 = $1.5M/month

SAVINGS: 69% reduction in storage costs!
```

-----

### **Storage Layout in S3:**

```
S3 BUCKET STRUCTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

s3://dropbox-chunks/
â”œâ”€â”€ chunks/
â”‚   â”œâ”€â”€ a3/              â† First 2 chars of hash (sharding)
â”‚   â”‚   â”œâ”€â”€ f5/          â† Next 2 chars
â”‚   â”‚   â”‚   â””â”€â”€ a3f5c8d... (full hash as filename)
â”‚   â”‚   â””â”€â”€ c8/
â”‚   â”‚       â””â”€â”€ a3c8d9e...
â”‚   â”œâ”€â”€ 7b/
â”‚   â”‚   â””â”€â”€ 2e/
â”‚   â”‚       â””â”€â”€ 7b2e9f1...
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ metadata/            â† Backup of metadata
    â””â”€â”€ shards/
        â”œâ”€â”€ shard_0/
        â””â”€â”€ shard_1/

BENEFITS:
âœ“ Uniform distribution (hash-based)
âœ“ Avoids S3 hot partitions
âœ“ Easy to parallelize operations
âœ“ Natural sharding for operations
```

-----

# **DEEP DIVE 2: Metadata Management (4 minutes)**

## **Part A: Metadata Schema Design (90 seconds)**

### **Complete Database Schema:**

```sql
-- Users table
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    username VARCHAR(100),
    storage_quota_gb INTEGER DEFAULT 10,
    storage_used_gb DECIMAL(12,2) DEFAULT 0,
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP,
    
    INDEX idx_email (email)
);

-- Files table (main metadata)
CREATE TABLE files (
    file_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    user_id BIGINT NOT NULL,
    parent_folder_id BIGINT,           -- NULL for root
    filename VARCHAR(512) NOT NULL,
    file_path VARCHAR(2048) NOT NULL,  -- Full path for quick lookup
    file_size_bytes BIGINT NOT NULL,
    file_type VARCHAR(50),              -- MIME type
    file_hash VARCHAR(64),              -- Hash of entire file
    version INTEGER DEFAULT 1,
    is_deleted BOOLEAN DEFAULT FALSE,
    is_shared BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW(),
    modified_at TIMESTAMP DEFAULT NOW(),
    last_accessed_at TIMESTAMP,
    
    -- Sharding key (critical!)
    SHARD_KEY: user_id
    
    INDEX idx_user_files (user_id, is_deleted, parent_folder_id),
    INDEX idx_path (user_id, file_path),
    INDEX idx_modified (user_id, modified_at),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);

-- Chunks table (already shown above)
CREATE TABLE chunks (
    chunk_hash VARCHAR(64) PRIMARY KEY,
    size_bytes INTEGER NOT NULL,
    storage_path VARCHAR(512) NOT NULL,
    ref_count INTEGER DEFAULT 1,
    created_at TIMESTAMP DEFAULT NOW(),
    last_accessed TIMESTAMP,
    
    INDEX idx_last_accessed (last_accessed)
);

-- File-to-chunks mapping
CREATE TABLE file_chunks (
    file_id BIGINT NOT NULL,
    chunk_hash VARCHAR(64) NOT NULL,
    chunk_index INTEGER NOT NULL,
    chunk_offset BIGINT NOT NULL,
    
    PRIMARY KEY (file_id, chunk_index),
    FOREIGN KEY (chunk_hash) REFERENCES chunks(chunk_hash),
    FOREIGN KEY (file_id) REFERENCES files(file_id) ON DELETE CASCADE,
    
    INDEX idx_chunk_lookup (chunk_hash)  -- For ref counting
);

-- File versions (for version history)
CREATE TABLE file_versions (
    version_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    file_id BIGINT NOT NULL,
    version_number INTEGER NOT NULL,
    file_size_bytes BIGINT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    created_by BIGINT NOT NULL,
    
    UNIQUE KEY unique_file_version (file_id, version_number),
    FOREIGN KEY (file_id) REFERENCES files(file_id) ON DELETE CASCADE
);

-- Version chunks mapping (what chunks this version has)
CREATE TABLE version_chunks (
    version_id BIGINT NOT NULL,
    chunk_hash VARCHAR(64) NOT NULL,
    chunk_index INTEGER NOT NULL,
    
    PRIMARY KEY (version_id, chunk_index),
    FOREIGN KEY (version_id) REFERENCES file_versions(version_id),
    FOREIGN KEY (chunk_hash) REFERENCES chunks(chunk_hash)
);

-- Sharing/permissions
CREATE TABLE file_permissions (
    permission_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    file_id BIGINT NOT NULL,
    shared_with_user_id BIGINT,        -- NULL for public links
    permission_type ENUM('read', 'write', 'owner'),
    share_link_token VARCHAR(64) UNIQUE,  -- For public links
    expires_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_user_permissions (shared_with_user_id),
    INDEX idx_share_token (share_link_token),
    FOREIGN KEY (file_id) REFERENCES files(file_id) ON DELETE CASCADE
);

-- Sync state (for each device)
CREATE TABLE device_sync_state (
    device_id VARCHAR(64) NOT NULL,
    user_id BIGINT NOT NULL,
    file_id BIGINT NOT NULL,
    local_version INTEGER NOT NULL,
    server_version INTEGER NOT NULL,
    sync_status ENUM('synced', 'pending', 'conflict'),
    last_sync_at TIMESTAMP,
    
    PRIMARY KEY (device_id, file_id),
    INDEX idx_user_device (user_id, device_id),
    FOREIGN KEY (user_id) REFERENCES users(user_id),
    FOREIGN KEY (file_id) REFERENCES files(file_id) ON DELETE CASCADE
);
```

-----

## **Part B: Database Sharding Strategy (90 seconds)**

### **Why Shard?**

> â€œWith 500M users and 100B files, a single database canâ€™t handle:
> 
> - 115K read QPS (even with caching)
> - 300TB of metadata
> - High availability requirements
> 
> We need horizontal sharding.â€

-----

### **Sharding Approach: User-Based Sharding**

```
SHARDING STRATEGY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Shard Key: user_id
Hash Function: Consistent Hashing
Number of Shards: 1,024 (configurable)

SHARD ASSIGNMENT:
shard_id = hash(user_id) % 1024

Example:
user_id = 123456
hash(123456) = 7483625
shard_id = 7483625 % 1024 = 329
â†’ User 123456 â†’ Shard 329
```

**Shard distribution:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARD 0 (100TB capacity)               â”‚
â”‚  - Users: 0-488K                        â”‚
â”‚  - Files: ~100M                         â”‚
â”‚  - Master + 2 Read Replicas             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SHARD 1                                â”‚
â”‚  - Users: 488K-976K                     â”‚
â”‚  - Files: ~100M                         â”‚
â”‚  - Master + 2 Read Replicas             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

... (1,022 more shards)
```

-----

### **Why User-Based Sharding?**

```
PROS:
âœ“ User queries never cross shards
  "Get all files for user_id=X" â†’ Single shard lookup
âœ“ Related data co-located (user's files together)
âœ“ Easy to maintain consistency per user
âœ“ Simple routing logic

CONS:
âœ— Power users create hot shards
  Solution: Detect and split hot users to dedicated shards
âœ— Shared files span shards
  Solution: Replicate sharing metadata
âœ— Chunk table not sharded by user
  Solution: Separate chunk table, global with caching
```

-----

### **Routing Layer:**

```python
class ShardRouter:
    def __init__(self, num_shards=1024):
        self.num_shards = num_shards
        self.shard_map = self._load_shard_map()
    
    def get_shard(self, user_id):
        """Route user to appropriate shard"""
        shard_id = hash(user_id) % self.num_shards
        return self.shard_map[shard_id]
    
    def _load_shard_map(self):
        """Load shard topology from config service"""
        # Returns: {shard_id: {'master': 'db1.host', 'replicas': [...]}}
        return load_from_config_service()

# Usage in API server
router = ShardRouter()

def get_user_files(user_id):
    shard = router.get_shard(user_id)
    db = connect_to_shard(shard['master'])
    
    return db.query("""
        SELECT * FROM files 
        WHERE user_id = ? AND is_deleted = FALSE
    """, user_id)
```

-----

### **Handling Cross-Shard Queries:**

```
PROBLEM: Shared files
User A (Shard 5) shares file with User B (Shard 7)

SOLUTION 1: Denormalize (Preferred)
- Store sharing record in BOTH shards
- file_permissions table replicated
- Each user queries own shard only

file_permissions in Shard 5:
  file_id | owner_id | shared_with | type
  --------|----------|-------------|------
  100     | A        | B           | read

file_permissions in Shard 7:
  file_id | owner_id | shared_with | type
  --------|----------|-------------|------
  100     | A        | B           | read  (replicated)

SOLUTION 2: Shared files service
- Separate microservice for shared files
- Handles cross-shard queries
- More complex but cleaner separation
```

-----

## **Part C: Caching Strategy (60 seconds)**

### **Multi-Layer Caching:**

```
CACHE HIERARCHY:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LAYER 1: Client-Side Cache
â”œâ”€ Location: Desktop/mobile app
â”œâ”€ What: File metadata, chunk hashes
â”œâ”€ TTL: Until file modified
â””â”€ Benefit: Zero network calls for repeated access

LAYER 2: CDN Cache (Varnish/CloudFront)
â”œâ”€ Location: Edge locations worldwide
â”œâ”€ What: Hot file chunks, thumbnails
â”œâ”€ TTL: 24 hours
â””â”€ Benefit: Low latency downloads

LAYER 3: Application Cache (Redis)
â”œâ”€ Location: Same datacenter as API servers
â”œâ”€ What: File metadata, user info, permissions
â”œâ”€ TTL: 5 minutes (frequent) to 1 hour (rare)
â””â”€ Benefit: Reduce DB load by 90%

LAYER 4: Database Query Cache
â”œâ”€ Location: PostgreSQL query cache
â”œâ”€ What: Query results
â”œâ”€ TTL: Automatic invalidation
â””â”€ Benefit: Repeated identical queries
```

-----

### **Redis Caching Implementation:**

```python
import redis
import json

redis_client = redis.Redis(host='cache.cluster', port=6379)

def get_file_metadata(file_id):
    """Get file metadata with caching"""
    cache_key = f"file:metadata:{file_id}"
    
    # Try cache first
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Cache miss - query database
    shard = router.get_shard_for_file(file_id)
    db = connect_to_shard(shard)
    
    file_data = db.query("""
        SELECT f.*, 
               GROUP_CONCAT(fc.chunk_hash ORDER BY fc.chunk_index) as chunks
        FROM files f
        LEFT JOIN file_chunks fc ON f.file_id = fc.file_id
        WHERE f.file_id = ?
        GROUP BY f.file_id
    """, file_id)
    
    if file_data:
        # Cache for 5 minutes
        redis_client.setex(
            cache_key, 
            300,  # TTL in seconds
            json.dumps(file_data)
        )
    
    return file_data

def invalidate_file_cache(file_id):
    """Invalidate cache when file changes"""
    cache_key = f"file:metadata:{file_id}"
    redis_client.delete(cache_key)
```

-----

### **Cache Invalidation Strategy:**

```
INVALIDATION PATTERNS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. WRITE-THROUGH
   Update database â†’ Update cache
   Pros: Cache always fresh
   Cons: Write latency increased

2. WRITE-BEHIND (Async)
   Update database â†’ Invalidate cache â†’ Lazy reload
   Pros: Fast writes
   Cons: Brief stale data possible

3. TTL-BASED (Our choice)
   Set expiration time on all cached data
   Pros: Simple, handles edge cases
   Cons: May serve stale data until expiry

DECISION: TTL-based with active invalidation
- Default TTL: 5 minutes
- On file modification: Active invalidation
- Balances consistency and performance
```

-----

# **DEEP DIVE 3: Synchronization Service (5 minutes)**

## **Part A: Sync Architecture (90 seconds)**

### **Overall Sync Flow:**

```
SYNC SERVICE ARCHITECTURE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          SYNC SERVICE CLUSTER            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Sync Node 1â”‚  â”‚ Sync Node 2â”‚  ...     â”‚
â”‚  â”‚            â”‚  â”‚            â”‚          â”‚
â”‚  â”‚ Manages    â”‚  â”‚ Manages    â”‚          â”‚
â”‚  â”‚ 100K       â”‚  â”‚ 100K       â”‚          â”‚
â”‚  â”‚ connectionsâ”‚  â”‚ connectionsâ”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Message Queue â”‚
         â”‚     (Kafka)     â”‚
         â”‚                 â”‚
         â”‚ Topics:         â”‚
         â”‚ - file.created  â”‚
         â”‚ - file.modified â”‚
         â”‚ - file.deleted  â”‚
         â”‚ - file.shared   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚           â”‚           â”‚
      â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Device A â”‚ â”‚Device B â”‚ â”‚Device C â”‚
â”‚(Online) â”‚ â”‚(Online) â”‚ â”‚(Offline)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†‘                         â”‚
    â””â”€â”€â”€â”€â”€â”€ Polls on â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           reconnect
```

-----

### **Connection Management:**

```python
class SyncService:
    def __init__(self):
        self.connections = {}  # device_id -> WebSocket
        self.user_devices = {}  # user_id -> [device_ids]
        
    async def handle_connection(self, websocket, device_id, user_id):
        """Handle new device connection"""
        
        # Register connection
        self.connections[device_id] = websocket
        
        if user_id not in self.user_devices:
            self.user_devices[user_id] = []
        self.user_devices[user_id].append(device_id)
        
        try:
            # Send initial sync
            await self.send_initial_sync(device_id, user_id)
            
            # Keep connection alive
            while True:
                # Heartbeat every 30 seconds
                await websocket.ping()
                await asyncio.sleep(30)
                
        except websockets.ConnectionClosed:
            # Clean up on disconnect
            self.cleanup_connection(device_id, user_id)
    
    async def send_initial_sync(self, device_id, user_id):
        """Send files that changed since last sync"""
        
        # Get device's last sync timestamp
        last_sync = get_device_last_sync(device_id)
        
        # Query files modified since last sync
        changed_files = query_db("""
            SELECT file_id, filename, file_hash, version, modified_at
            FROM files
            WHERE user_id = ? AND modified_at > ?
        """, user_id, last_sync)
        
        # Send to device
        await self.send_sync_message(device_id, {
            'type': 'initial_sync',
            'files': changed_files,
            'timestamp': now()
        })
```

-----

## **Part B: Change Detection & Propagation (90 seconds)**

### **How Changes Are Detected:**

```
CLIENT-SIDE CHANGE DETECTION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. FILE SYSTEM WATCHER
   - OS-level hooks (inotify on Linux, FSEvents on Mac)
   - Detects: CREATE, MODIFY, DELETE, RENAME events
   
2. CHANGE DEBOUNCING
   - User saves file 10x in 5 seconds
   - Don't sync each save!
   - Wait for 2 seconds of inactivity
   - Then trigger sync

3. CALCULATE DELTA
   - Read modified file
   - Chunk it (content-defined chunking)
   - Compare hashes with last synced version
   - Identify changed chunks only

EXAMPLE:
File: presentation.pptx (100 MB)
User modifies slide 3 (affects 2 chunks: chunk[5], chunk[6])

OLD VERSION CHUNKS:
[chunk0][chunk1][chunk2]...[chunk5][chunk6]...[chunk24]
                            â†“       â†“
                         (old)    (old)

NEW VERSION CHUNKS:
[chunk0][chunk1][chunk2]...[chunk5'][chunk6']...[chunk24]
                            â†“        â†“
                          (new)    (new)

DELTA SYNC:
- Upload only: chunk5', chunk6' (8 MB instead of 100 MB!)
- Saves 92% bandwidth
```

-----

### **Server-Side Change Propagation:**

```python
async def handle_file_upload(user_id, file_id, chunks, version):
    """Handle file upload and propagate to other devices"""
    
    # 1. Validate version (optimistic locking)
    current_version = get_file_version(file_id)
    if version != current_version:
        # Conflict detected!
        return {'status': 'conflict', 'current_version': current_version}
    
    # 2. Store new chunks
    for chunk in chunks:
        store_chunk_if_new(chunk)
    
    # 3. Update metadata
    new_version = current_version + 1
    update_file_metadata(fileâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹_id, new_version, chunks)

    # 4. Publish change event to Kafka
    await kafka_producer.send('file.modified', {
        'user_id': user_id,
        'file_id': file_id,
        'version': new_version,
        'modified_by_device': request.device_id,
        'timestamp': now(),
        'changed_chunks': [c['hash'] for c in chunks]
    })

    # 5. Sync service consumes event and notifies devices
    # (happens asynchronously)

    return {'status': 'success', 'version': new_version}

# Sync service consumer
async def process_file_change_event(event):
    """Process file change and notify all userâ€™s devices"""

    user_id = event['user_id']
    file_id = event['file_id']
    source_device = event['modified_by_device']

    # Get all devices for this user (except the source)
    target_devices = [
        d for d in get_user_devices(user_id) 
        if d != source_device
    ]

    # Send notification to each online device
    for device_id in target_devices:
        if device_id in self.connections:
            # Device is online - push immediately
            await self.send_sync_message(device_id, {
                'type': 'file_changed',
                'file_id': file_id,
                'version': event['version'],
                'changed_chunks': event['changed_chunks']
            })
        else:
            # Device offline - mark for next sync
            mark_device_needs_sync(device_id, file_id)
```

-----

## **Part C: Conflict Resolution (90 seconds)**

### **Conflict Scenarios:**

```
CONFLICT TYPES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. SIMPLE CONFLICT (Most Common)
   Time: T0                T1              T2
   Device A: Edit file â†’ Upload (v2)
   Device B:             Edit file    â†’ Upload (v2) âŒ
   
   Result: Device B has stale version!
   Resolution: Last Write Wins
1. SIMULTANEOUS EDITS (Rare but happens)
   Time: T0              T1
   Device A: Edit â†’ Upload (arrives first)  â†’ v2
   Device B: Edit â†’ Upload (arrives second) â†’ conflict!
   
   Result: Both edited same version
   Resolution: Create conflict copy
1. OFFLINE EDITING
   Device A: Online, edits file â†’ v2, v3, v4
   Device B: Offline for 2 days, edits v1 â†’ wants to upload
   
   Result: Device B is far behind
   Resolution: Create conflict copy + notify user
```

-----

### **Conflict Resolution Strategy:**

```
VERSIONING SYSTEM:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Each file has:

- version_number: Integer, monotonically increasing
- last_modified_timestamp: For tie-breaking
- last_modified_by: Device ID

ALGORITHM:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ On Upload Request:                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ IF client_version == server_version:    â”‚
â”‚    â†’ Accept upload                      â”‚
â”‚    â†’ Increment version                  â”‚
â”‚    â†’ Notify other devices               â”‚
â”‚                                         â”‚
â”‚ ELSE IF client_version < server_version:â”‚
â”‚    â†’ Reject upload                      â”‚
â”‚    â†’ Return conflict error              â”‚
â”‚    â†’ Client must resolve                â”‚
â”‚                                         â”‚
â”‚ ELSE: (shouldnâ€™t happen)                â”‚
â”‚    â†’ Log error                          â”‚
â”‚    â†’ Manual intervention                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CLIENT-SIDE RESOLUTION:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ On Conflict Error:                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Download latest version from server  â”‚
â”‚ 2. Save local version as:               â”‚
â”‚    â€œdocument (conflicted copy).txtâ€     â”‚
â”‚ 3. Show notification to user:           â”‚
â”‚    â€œConflict detected - review copiesâ€  â”‚
â”‚ 4. User manually merges if needed       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

-----

### **Implementation with Vector Clocks (Advanced):**

```python
class VectorClock:
    """
    More sophisticated conflict detection
    Used by systems like Dropbox
    """
    def __init__(self):
        self.clock = {}  # device_id -> counter
    
    def increment(self, device_id):
        """Increment this device's counter"""
        self.clock[device_id] = self.clock.get(device_id, 0) + 1
    
    def merge(self, other_clock):
        """Merge two vector clocks (for conflict detection)"""
        for device_id, counter in other_clock.items():
            self.clock[device_id] = max(
                self.clock.get(device_id, 0),
                counter
            )
    
    def happens_before(self, other_clock):
        """Check if this clock happened before other"""
        # This < Other if all counters <= and at least one <
        all_lte = all(
            self.clock.get(d, 0) <= other_clock.get(d, 0)
            for d in set(self.clock) | set(other_clock)
        )
        any_lt = any(
            self.clock.get(d, 0) < other_clock.get(d, 0)
            for d in set(self.clock) | set(other_clock)
        )
        return all_lte and any_lt
    
    def is_concurrent(self, other_clock):
        """Check if two clocks are concurrent (conflict!)"""
        return (not self.happens_before(other_clock) and 
                not other_clock.happens_before(self))

# Usage in conflict detection
def detect_conflict(file_id, client_clock, client_chunks):
    """Detect conflicts using vector clocks"""
    
    # Get server's vector clock for this file
    server_clock = get_file_vector_clock(file_id)
    
    if client_clock.happens_before(server_clock):
        # Client is behind - reject upload
        return {
            'conflict': True,
            'resolution': 'reject',
            'message': 'File was modified by another device'
        }
    
    elif server_clock.happens_before(client_clock):
        # Client is ahead - accept upload (normal case)
        return {
            'conflict': False,
            'resolution': 'accept'
        }
    
    elif client_clock.is_concurrent(server_clock):
        # Concurrent edits - conflict!
        return {
            'conflict': True,
            'resolution': 'create_copy',
            'message': 'Conflicting changes detected'
        }

EXAMPLE:
Device A clock: {A: 5, B: 2, C: 1}
Device B clock: {A: 4, B: 3, C: 1}

Compare:
- A: 5 vs 4 (A ahead)
- B: 2 vs 3 (B ahead)
- C: 1 vs 1 (equal)

Result: Concurrent! â†’ Conflict
```

-----

## **Part D: Real-Time Synchronization Performance (60 seconds)**

### **WebSocket Connection Scaling:**

```
CONNECTION SCALING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHALLENGE: 100M DAU, assume 2 devices each
           = 200M concurrent WebSocket connections

SOLUTION: Connection Pooling per Sync Node

Each Sync Node (server):
- Handles 100,000 concurrent connections
- Requires ~200GB RAM (2KB per connection)
- Uses epoll/kqueue for efficient I/O

Total Sync Nodes Needed:
200M connections / 100K per node = 2,000 nodes

With 2x redundancy: 4,000 sync nodes

OPTIMIZATION: Not all users online simultaneously
- Peak online: 20% of DAU = 20M users
- Active connections: 40M (2 devices per user)
- Sync nodes needed: 400 nodes
- With 2x redundancy: 800 nodes
```

-----

### **Message Queue Throughput:**

```
KAFKA CONFIGURATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Topics:
- file.created (low volume)
- file.modified (high volume)
- file.deleted (low volume)
- file.shared (medium volume)

Partitioning Strategy:
- Partition by user_id (hash-based)
- 1,000 partitions per topic
- Ensures all user's events ordered
- Parallel processing across consumers

Throughput:
- Upload QPS: 7,000 (peak)
- Each upload â†’ 1 event
- Events per second: 7,000 EPS

Kafka capacity:
- Modern Kafka: 1M messages/sec per cluster
- Our need: 7K messages/sec
- Utilization: 0.7% (lots of headroom!)

Consumer Groups:
- Sync Service: 100 consumers
- Analytics: 50 consumers
- Each consumer handles ~70 events/sec
```

-----

### **Latency Breakdown:**

```
SYNC LATENCY TARGET: < 10 seconds
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ACTUAL LATENCY BREAKDOWN:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step                        â”‚ Time      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. File watcher detects     â”‚ ~500ms    â”‚
â”‚    change (debouncing)      â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Calculate delta chunks   â”‚ ~200ms    â”‚
â”‚    (for 10MB file)          â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Upload chunks to S3      â”‚ ~2s       â”‚
â”‚    (network latency)        â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Update metadata DB       â”‚ ~50ms     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. Publish to Kafka         â”‚ ~10ms     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6. Kafka â†’ Sync Service     â”‚ ~50ms     â”‚
â”‚    (consumer lag)           â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. WebSocket push           â”‚ ~100ms    â”‚
â”‚    to device                â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8. Device downloads delta   â”‚ ~2s       â”‚
â”‚    chunks                   â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 9. Device applies changes   â”‚ ~200ms    â”‚
â”‚                             â”‚           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TOTAL:                      â”‚ ~5.1s âœ“   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Result: Well under 10 second target!
```

-----

# **Summary of Deep Dive Components (30 seconds)**

**Quickly recap the three deep dives:**

> â€œLet me summarize the core components weâ€™ve covered:

**1. File Chunking & Storage:**

- 4MB chunks using content-defined chunking
- SHA-256 hashing for deduplication
- Multi-tier storage (hot/warm/cold)
- Reference counting for garbage collection

**2. Metadata Management:**

- Sharded PostgreSQL by user_id
- 300TB metadata across 1,024 shards
- Redis caching with 90% hit rate
- TTL-based invalidation

**3. Synchronization:**

- WebSocket connections for real-time sync
- Kafka for event propagation
- Vector clocks for conflict detection
- < 5 second sync latency

These three components work together to provide fast, reliable file storage and synchronization at massive scale.â€

-----

# **Transition Questions (15 seconds)**

**Ask the interviewer:**

> â€œWeâ€™ve covered the core technical components in depth. Would you like me to:
> 
> A) Dive into advanced features like sharing, permissions, or versioning?
> B) Discuss scalability bottlenecks and how to address them?
> C) Talk about security and encryption?
> D) Cover monitoring and observability?
> 
> Or is there a specific component youâ€™d like me to elaborate on further?â€

-----

# **Common Deep Dive Mistakes to Avoid:**

âŒ **Too shallow** - â€œWe use chunksâ€ without explaining why or how

âŒ **Too much code** - Donâ€™t write full implementations, pseudo-code is enough

âŒ **No trade-offs** - Every decision has pros/cons, mention them

âŒ **Ignoring scale** - â€œWeâ€™ll use a single databaseâ€ â†’ Shows lack of experience

âŒ **Over-engineering** - Donâ€™t propose blockchain for conflict resolution

âŒ **No numbers** - â€œItâ€™ll be fastâ€ vs â€œ5 second latency under p99â€

âŒ **Forgetting edge cases** - What if user uploads same file 1000x?

âŒ **No failure scenarios** - What happens when Kafka is down?

-----

# **Pro Tips for Deep Dives:**

âœ… **Use real-world examples:**

- â€œSimilar to how Dropbox handles thisâ€¦â€
- â€œAmazon S3 uses this approachâ€¦â€
- Shows youâ€™ve studied production systems

âœ… **Quantify everything:**

- Not â€œfastâ€ but â€œ< 100ms p99 latencyâ€
- Not â€œlots of connectionsâ€ but â€œ100K per nodeâ€

âœ… **Draw while explaining:**

- Schemas on whiteboard
- Data flow diagrams
- State transitions

âœ… **Acknowledge alternatives:**

- â€œWe could use MySQL or Cassandra. MySQL gives us ACIDâ€¦â€
- Shows breadth of knowledge

âœ… **Connect to earlier sections:**

- â€œRemember we calculated 700K QPS? Thatâ€™s why we need this caching layerâ€
- Shows coherent thinking

âœ… **Prepare for follow-ups:**

- Interviewer might ask: â€œWhat if deduplication has hash collision?â€
- Have answers ready for obvious questions

âœ… **Be honest about limitations:**

- â€œThis approach works for files under 10GB. For larger files, weâ€™d need toâ€¦â€
- Better than pretending youâ€™ve solved everything

-----

# **What Your Whiteboard Should Look Like After Deep Dives:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEEP DIVE: FILE CHUNKING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 4MB chunks (content-defined, Rabin fingerprinting)    â”‚
â”‚ â€¢ SHA-256 for deduplication (67% storage savings)       â”‚
â”‚ â€¢ Reference counting in chunks table                    â”‚
â”‚ â€¢ Multi-tier: Hot (S3) â†’ Warm (IA) â†’ Cold (Glacier)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEEP DIVE: METADATA DB                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ PostgreSQL sharded by user_id (1,024 shards)          â”‚
â”‚ â€¢ Schema: users, files, chunks, file_chunks, versions   â”‚
â”‚ â€¢ Redis cache (90% hit rate, 5 min TTL)                 â”‚
â”‚ â€¢ Reduces DB load: 1.15M â†’ 115K QPS                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DEEP DIVE: SYNC SERVICE                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ WebSocket connections (800 nodes, 100K conn/node)     â”‚
â”‚ â€¢ Kafka for events (7K EPS, 1000 partitions)            â”‚
â”‚ â€¢ Vector clocks for conflict detection                  â”‚
â”‚ â€¢ Total latency: ~5 seconds (target: <10s) âœ“            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

-----

# **Interviewer Signals to Watch:**

**Good signs:**

- ğŸ˜Š Nodding along
- ğŸ¤” Asking detailed follow-ups
- âœï¸ Taking notes
- ğŸ’¬ â€œInteresting approachâ€¦â€
- â° Letting you talk longer than planned

**Warning signs:**

- ğŸ˜ Blank stare (youâ€™ve lost them)
- â­ï¸ â€œLetâ€™s move onâ€¦â€ (theyâ€™re not interested)
- ğŸ“± Checking time frequently (too slow)
- ğŸ¤¨ â€œAre you sure about that?â€ (you made an error)

**Adjust accordingly:**

- If lost â†’ slow down, use simpler examples
- If bored â†’ speed up, move to next component
- If engaged â†’ go deeper, show more expertise

-----

# **Time Check:**

At 35 minutes, you should have:

- âœ… Gathered requirements (5 min)
- âœ… Done capacity estimates (5 min)
- âœ… Drawn high-level design (10 min)
- âœ… Deep-dived 3 components (15 min)

**Remaining: 25 minutes**

**Whatâ€™s next:**

- Advanced features (10 min)
- Scalability & bottlenecks (7 min)
- Trade-offs (5 min)
- Wrap-up (3 min)

Youâ€™re right on schedule! ğŸ¯â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹
